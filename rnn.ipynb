{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Positive Activating</td>\n",
       "      <td>I found another interesting clue in your first...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Personal advice</td>\n",
       "      <td>It is good to see more details on this.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vocatives (addressing individual)</td>\n",
       "      <td>Hi Daniel,Thanks for your comments.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Positive Deactivating</td>\n",
       "      <td>This is cool!This confirms my personal impress...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Complementing, expressing appreciation</td>\n",
       "      <td>The final result is quite compelling and reall...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     code  \\\n",
       "0                     Positive Activating   \n",
       "1                         Personal advice   \n",
       "2       Vocatives (addressing individual)   \n",
       "3                   Positive Deactivating   \n",
       "4  Complementing, expressing appreciation   \n",
       "\n",
       "                                                text  \n",
       "0  I found another interesting clue in your first...  \n",
       "1            It is good to see more details on this.  \n",
       "2                Hi Daniel,Thanks for your comments.  \n",
       "3  This is cool!This confirms my personal impress...  \n",
       "4  The final result is quite compelling and reall...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "code\n",
       "Addresses or refers to the group using inclusive pronouns (addressing the whole group)     15\n",
       "Asking questions                                                                          180\n",
       "Complementing, expressing appreciation                                                    371\n",
       "Course reflection                                                                           2\n",
       "Expressing agreement                                                                       79\n",
       "Expressing disagreement                                                                    28\n",
       "Group cohesion                                                                              1\n",
       "Negative Activating                                                                       157\n",
       "Negative Deactivating                                                                      67\n",
       "Open communication                                                                          2\n",
       "Personal advice                                                                           202\n",
       "Phatics, salutations and greetings, Social sharing                                         66\n",
       "Positive Activating                                                                       220\n",
       "Positive Deactivating                                                                      30\n",
       "Quoting from others' messages & Referring explicitly to others' messages                   43\n",
       "Self-disclosure & expressing values                                                        57\n",
       "Unconventional emotion expression                                                          56\n",
       "Vocatives (addressing individual)                                                         183\n",
       "Name: text, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('code')['text'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "code\n",
       "Asking questions                          180\n",
       "Complementing, expressing appreciation    371\n",
       "Negative Activating                       157\n",
       "Personal advice                           202\n",
       "Vocatives (addressing individual)         183\n",
       "Name: text, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# over_50_cols = [\n",
    "#     'Asking questions', 'Complementing, expressing appreciation', 'Expressing agreement',\n",
    "#     'Negative Activating', 'Negative Deactivating', 'Personal advice',\n",
    "#     'Phatics, salutations and greetings, Social sharing', 'Positive Activating',\n",
    "#     'Self-disclosure & expressing values', 'Unconventional emotion expression', 'Vocatives (addressing individual)'\n",
    "# ]\n",
    "over_100_cols = [\n",
    "    'Asking questions', 'Complementing, expressing appreciation',\n",
    "    'Negative Activating', 'Personal advice',\n",
    "    'Vocatives (addressing individual)'\n",
    "]\n",
    "df_over_50 = df[df['code'].isin(over_100_cols)]\n",
    "df_over_50.groupby('code')['text'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.to_csv('./filtered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def standardize_text(df, text_field):\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\\S+\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\\S+\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]\", \" \")\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\", \"at\")\n",
    "    \n",
    "    df[text_field] = df[text_field].str.lower()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chengluli/anaconda2/envs/stan/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chengluli/anaconda2/envs/stan/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/chengluli/anaconda2/envs/stan/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "/Users/chengluli/anaconda2/envs/stan/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/chengluli/anaconda2/envs/stan/lib/python3.7/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "/Users/chengluli/anaconda2/envs/stan/lib/python3.7/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Personal advice</td>\n",
       "      <td>it is good to see more details on this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vocatives (addressing individual)</td>\n",
       "      <td>hi daniel,thanks for your comments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Complementing, expressing appreciation</td>\n",
       "      <td>the final result is quite compelling and reall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Negative Activating</td>\n",
       "      <td>i love the lead and the conclusion drawn by th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>did you find any similarities about these two ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     code  \\\n",
       "1                         Personal advice   \n",
       "2       Vocatives (addressing individual)   \n",
       "4  Complementing, expressing appreciation   \n",
       "5                     Negative Activating   \n",
       "6                        Asking questions   \n",
       "\n",
       "                                                text  \n",
       "1            it is good to see more details on this   \n",
       "2                hi daniel,thanks for your comments   \n",
       "4  the final result is quite compelling and reall...  \n",
       "5  i love the lead and the conclusion drawn by th...  \n",
       "6  did you find any similarities about these two ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned = standardize_text(df_over_50, 'text')\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chengluli/anaconda2/envs/stan/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "df_cleaned[\"tokens\"] = df_cleaned[\"text\"].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13386 words total, with a vocabulary size of 2135\n",
      "Max sentence length is 55\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "all_words = [word for tokens in df_cleaned[\"tokens\"] for word in tokens]\n",
    "sentence_lengths = [len(tokens) for tokens in df_cleaned[\"tokens\"]]\n",
    "VOCAB = sorted(list(set(all_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_words), len(VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "word2vec_path = \"GoogleNews-vectors-negative300.bin.gz\"\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
    "    if len(tokens_list) < 1:\n",
    "        return np.zeros(k)\n",
    "    if generate_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged\n",
    "\n",
    "def get_word2vec_embeddings(vectors, clean_questions, generate_missing=False):\n",
    "    embeddings = clean_questions['tokens'].apply(lambda x: get_average_word2vec(x, vectors, \n",
    "                                                                                generate_missing=generate_missing))\n",
    "    return list(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = get_word2vec_embeddings(word2vec, df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras as K\n",
    "from keras.layers import *\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "    \n",
    "\n",
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "    Note: The layer has been tested with Keras 2.0.6\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2135 unique tokens.\n",
      "(2136, 300)\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "MAX_SEQUENCE_LENGTH = max(sentence_lengths)\n",
    "VOCAB_SIZE = len(VOCAB)\n",
    "\n",
    "VALIDATION_SPLIT=.2\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(df_cleaned[\"text\"].tolist())\n",
    "sequences = tokenizer.texts_to_sequences(df_cleaned[\"text\"].tolist())\n",
    "\n",
    "rnn_data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "factorized_codes = pd.factorize(df_cleaned[\"code\"])[0]\n",
    "factorized_codes_dict = pd.factorize(df_cleaned[\"code\"])[1]\n",
    "\n",
    "labels = factorized_codes\n",
    "\n",
    "indices = np.arange(rnn_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "rnn_data = rnn_data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * rnn_data.shape[0])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "embedding_weights = np.zeros((len(word_index)+1, EMBEDDING_DIM))\n",
    "for word, index in word_index.items():\n",
    "    embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "print(embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('tokenizer.json', 'w') as outfile:  \n",
    "    json.dump(tokenizer.word_index, fp=outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras_metrics as km\n",
    "sc_precision = km.sparse_categorical_precision()\n",
    "sc_recall = km.sparse_categorical_recall()\n",
    "\n",
    "def rnn_text(embeddings, max_sequence_length, num_words, embedding_dim, labels_index): \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_words,\n",
    "                        embedding_dim,\n",
    "                        weights=[embeddings],\n",
    "                        input_length=max_sequence_length,\n",
    "                        mask_zero=True,\n",
    "                        trainable=False))\n",
    "    # model.add(Bidirectional(LSTM(100)))\n",
    "    model.add(LSTM(128, return_sequences=True))\n",
    "#     model.add(LSTM(64, return_sequences=True))\n",
    "    model.add(LSTM(64, return_sequences=True))\n",
    "    # Attention layer not used because tfjs cannot load custom layer\n",
    "    # need to figure out how\n",
    "    model.add(AttentionWithContext())\n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(32, activation=\"relu\"))\n",
    "    model.add(Dense(labels_index, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=[sc_precision, sc_recall, 'accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = rnn_data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = rnn_data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "\n",
    "rnn_model = rnn_text(\n",
    "    embedding_weights, \n",
    "    MAX_SEQUENCE_LENGTH, \n",
    "    len(word_index)+1, \n",
    "    EMBEDDING_DIM, \n",
    "    len(list(df_cleaned[\"code\"].unique()))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 4s 5ms/step - loss: 1.5751 - precision: 0.0000e+00 - recall: 0.0000e+00 - acc: 0.3291 - val_loss: 1.5240 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_acc: 0.3257\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      " 64/875 [=>............................] - ETA: 1s - loss: 1.5036 - precision: 0.0000e+00 - recall: 0.0000e+00 - acc: 0.3438"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chengluli/anaconda2/envs/stan/lib/python3.7/site-packages/ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "875/875 [==============================] - 2s 3ms/step - loss: 1.4071 - precision: 0.0000e+00 - recall: 0.0000e+00 - acc: 0.4080 - val_loss: 1.2530 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_acc: 0.5183\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 1.1666 - precision: 0.6842 - recall: 0.0867 - acc: 0.4949 - val_loss: 1.1353 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_acc: 0.5092\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 1.0364 - precision: 0.7143 - recall: 0.1333 - acc: 0.5406 - val_loss: 1.0655 - val_precision: 0.8947 - val_recall: 0.3269 - val_acc: 0.5413\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.9703 - precision: 0.6383 - recall: 0.4000 - acc: 0.5383 - val_loss: 0.8775 - val_precision: 0.7879 - val_recall: 0.5000 - val_acc: 0.6376\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.8671 - precision: 0.7476 - recall: 0.5133 - acc: 0.6023 - val_loss: 0.8397 - val_precision: 0.8889 - val_recall: 0.4615 - val_acc: 0.6651\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.7825 - precision: 0.8175 - recall: 0.6867 - acc: 0.6594 - val_loss: 0.7600 - val_precision: 0.8837 - val_recall: 0.7308 - val_acc: 0.6743\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.7160 - precision: 0.7883 - recall: 0.7200 - acc: 0.6949 - val_loss: 0.8022 - val_precision: 0.8222 - val_recall: 0.7115 - val_acc: 0.6606\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.6632 - precision: 0.8079 - recall: 0.8133 - acc: 0.7291 - val_loss: 0.7909 - val_precision: 0.8684 - val_recall: 0.6346 - val_acc: 0.6606\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.6232 - precision: 0.8560 - recall: 0.7133 - acc: 0.7417 - val_loss: 0.7617 - val_precision: 0.7692 - val_recall: 0.7692 - val_acc: 0.7110\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.6148 - precision: 0.8264 - recall: 0.7933 - acc: 0.7589 - val_loss: 0.7874 - val_precision: 0.8462 - val_recall: 0.6346 - val_acc: 0.6743\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.5757 - precision: 0.8760 - recall: 0.7533 - acc: 0.7623 - val_loss: 0.7291 - val_precision: 0.8163 - val_recall: 0.7692 - val_acc: 0.7202\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.5008 - precision: 0.8667 - recall: 0.8667 - acc: 0.8011 - val_loss: 0.7713 - val_precision: 0.8125 - val_recall: 0.7500 - val_acc: 0.7202\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.4869 - precision: 0.9000 - recall: 0.8400 - acc: 0.8183 - val_loss: 0.7308 - val_precision: 0.9048 - val_recall: 0.7308 - val_acc: 0.7156\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.4627 - precision: 0.8750 - recall: 0.8867 - acc: 0.8183 - val_loss: 0.8111 - val_precision: 0.8163 - val_recall: 0.7692 - val_acc: 0.7156\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.4075 - precision: 0.9231 - recall: 0.8800 - acc: 0.8514 - val_loss: 0.8287 - val_precision: 0.7857 - val_recall: 0.8462 - val_acc: 0.7064\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.3901 - precision: 0.8940 - recall: 0.9000 - acc: 0.8389 - val_loss: 0.8154 - val_precision: 0.8077 - val_recall: 0.8077 - val_acc: 0.7110\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.3665 - precision: 0.9205 - recall: 0.9267 - acc: 0.8389 - val_loss: 0.8119 - val_precision: 0.9111 - val_recall: 0.7885 - val_acc: 0.7202\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.3526 - precision: 0.9145 - recall: 0.9267 - acc: 0.8594 - val_loss: 0.9257 - val_precision: 0.8163 - val_recall: 0.7692 - val_acc: 0.7064\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.3348 - precision: 0.9384 - recall: 0.9133 - acc: 0.8571 - val_loss: 0.8015 - val_precision: 0.8431 - val_recall: 0.8269 - val_acc: 0.6972\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.3477 - precision: 0.9333 - recall: 0.9333 - acc: 0.8366 - val_loss: 0.8489 - val_precision: 0.9024 - val_recall: 0.7115 - val_acc: 0.7156\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.3442 - precision: 0.9456 - recall: 0.9267 - acc: 0.8469 - val_loss: 0.8635 - val_precision: 0.8864 - val_recall: 0.7500 - val_acc: 0.7156\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.3196 - precision: 0.9404 - recall: 0.9467 - acc: 0.8629 - val_loss: 0.9935 - val_precision: 0.7636 - val_recall: 0.8077 - val_acc: 0.6835\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.3124 - precision: 0.9333 - recall: 0.9333 - acc: 0.8491 - val_loss: 0.9070 - val_precision: 0.7895 - val_recall: 0.8654 - val_acc: 0.7202\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.4220 - precision: 0.8467 - recall: 0.8467 - acc: 0.8149 - val_loss: 1.0612 - val_precision: 0.9474 - val_recall: 0.3462 - val_acc: 0.6147\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.4028 - precision: 0.8716 - recall: 0.8600 - acc: 0.8263 - val_loss: 0.8372 - val_precision: 0.8750 - val_recall: 0.6731 - val_acc: 0.7110\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.2885 - precision: 0.9524 - recall: 0.9333 - acc: 0.8594 - val_loss: 0.9840 - val_precision: 0.8367 - val_recall: 0.7885 - val_acc: 0.7156\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.2817 - precision: 0.9267 - recall: 0.9267 - acc: 0.8686 - val_loss: 1.0665 - val_precision: 0.9211 - val_recall: 0.6731 - val_acc: 0.7110\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.2952 - precision: 0.9262 - recall: 0.9200 - acc: 0.8663 - val_loss: 1.2047 - val_precision: 0.9655 - val_recall: 0.5385 - val_acc: 0.6697\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.3126 - precision: 0.9452 - recall: 0.9200 - acc: 0.8571 - val_loss: 1.2721 - val_precision: 0.8718 - val_recall: 0.6538 - val_acc: 0.6697\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.2698 - precision: 0.9533 - recall: 0.9533 - acc: 0.8800 - val_loss: 0.9342 - val_precision: 0.8431 - val_recall: 0.8269 - val_acc: 0.7064\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.2554 - precision: 0.9597 - recall: 0.9533 - acc: 0.8777 - val_loss: 0.9586 - val_precision: 0.8511 - val_recall: 0.7692 - val_acc: 0.7018\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.2492 - precision: 0.9533 - recall: 0.9533 - acc: 0.8651 - val_loss: 0.9767 - val_precision: 0.8776 - val_recall: 0.8269 - val_acc: 0.7202\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.2324 - precision: 0.9467 - recall: 0.9467 - acc: 0.8766 - val_loss: 0.9983 - val_precision: 0.8039 - val_recall: 0.7885 - val_acc: 0.7064\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.2248 - precision: 0.9730 - recall: 0.9600 - acc: 0.8789 - val_loss: 1.0911 - val_precision: 0.9512 - val_recall: 0.7500 - val_acc: 0.7110\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.2166 - precision: 0.9931 - recall: 0.9533 - acc: 0.8891 - val_loss: 1.0558 - val_precision: 0.8235 - val_recall: 0.8077 - val_acc: 0.6927\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.2175 - precision: 0.9730 - recall: 0.9600 - acc: 0.8766 - val_loss: 1.0818 - val_precision: 0.9111 - val_recall: 0.7885 - val_acc: 0.7202\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.3103 - precision: 0.9459 - recall: 0.9333 - acc: 0.8731 - val_loss: 1.2094 - val_precision: 0.8571 - val_recall: 0.5769 - val_acc: 0.6651\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.3091 - precision: 0.9320 - recall: 0.9133 - acc: 0.8446 - val_loss: 1.0901 - val_precision: 0.9091 - val_recall: 0.5769 - val_acc: 0.6606\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.2607 - precision: 0.9533 - recall: 0.9533 - acc: 0.8674 - val_loss: 1.0144 - val_precision: 0.8810 - val_recall: 0.7115 - val_acc: 0.7110\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.2360 - precision: 0.9859 - recall: 0.9333 - acc: 0.8766 - val_loss: 1.0308 - val_precision: 0.8431 - val_recall: 0.8269 - val_acc: 0.7156\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.2215 - precision: 0.9539 - recall: 0.9667 - acc: 0.8903 - val_loss: 1.1235 - val_precision: 0.9070 - val_recall: 0.7500 - val_acc: 0.7248\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.2014 - precision: 0.9533 - recall: 0.9533 - acc: 0.8800 - val_loss: 1.5861 - val_precision: 0.9615 - val_recall: 0.4808 - val_acc: 0.6514\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.2172 - precision: 0.9655 - recall: 0.9333 - acc: 0.8743 - val_loss: 1.2487 - val_precision: 0.8810 - val_recall: 0.7115 - val_acc: 0.7064\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.2125 - precision: 0.9660 - recall: 0.9467 - acc: 0.8926 - val_loss: 1.0962 - val_precision: 0.9302 - val_recall: 0.7692 - val_acc: 0.7156\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.2038 - precision: 0.9530 - recall: 0.9467 - acc: 0.8834 - val_loss: 1.2396 - val_precision: 0.8667 - val_recall: 0.7500 - val_acc: 0.7018\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1877 - precision: 0.9864 - recall: 0.9667 - acc: 0.8971 - val_loss: 1.1998 - val_precision: 0.8400 - val_recall: 0.8077 - val_acc: 0.7064\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1811 - precision: 0.9797 - recall: 0.9667 - acc: 0.8971 - val_loss: 1.3811 - val_precision: 0.9000 - val_recall: 0.6923 - val_acc: 0.6927\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.2313 - precision: 0.9583 - recall: 0.9200 - acc: 0.8880 - val_loss: 1.7045 - val_precision: 0.9500 - val_recall: 0.3654 - val_acc: 0.6376\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.3711 - precision: 0.8298 - recall: 0.7800 - acc: 0.8366 - val_loss: 1.4244 - val_precision: 0.8571 - val_recall: 0.3462 - val_acc: 0.6330\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.2934 - precision: 0.8742 - recall: 0.8800 - acc: 0.8537 - val_loss: 1.2767 - val_precision: 0.9189 - val_recall: 0.6538 - val_acc: 0.6560\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.2262 - precision: 0.9658 - recall: 0.9400 - acc: 0.8789 - val_loss: 1.0671 - val_precision: 0.8511 - val_recall: 0.7692 - val_acc: 0.7202\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.2075 - precision: 0.9726 - recall: 0.9467 - acc: 0.8891 - val_loss: 1.1825 - val_precision: 0.8696 - val_recall: 0.7692 - val_acc: 0.7018\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1936 - precision: 0.9724 - recall: 0.9400 - acc: 0.8937 - val_loss: 1.2364 - val_precision: 0.8478 - val_recall: 0.7500 - val_acc: 0.7110\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1818 - precision: 0.9730 - recall: 0.9600 - acc: 0.9051 - val_loss: 1.2582 - val_precision: 0.8542 - val_recall: 0.7885 - val_acc: 0.7110\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1769 - precision: 0.9797 - recall: 0.9667 - acc: 0.8891 - val_loss: 1.3660 - val_precision: 0.8367 - val_recall: 0.7885 - val_acc: 0.7064\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 4ms/step - loss: 0.1746 - precision: 0.9662 - recall: 0.9533 - acc: 0.8914 - val_loss: 1.3570 - val_precision: 0.8269 - val_recall: 0.8269 - val_acc: 0.6881\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 4ms/step - loss: 0.1613 - precision: 0.9864 - recall: 0.9667 - acc: 0.8994 - val_loss: 1.5362 - val_precision: 0.8367 - val_recall: 0.7885 - val_acc: 0.6789\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1657 - precision: 0.9733 - recall: 0.9733 - acc: 0.9040 - val_loss: 1.4886 - val_precision: 0.8478 - val_recall: 0.7500 - val_acc: 0.6927\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1916 - precision: 0.9862 - recall: 0.9533 - acc: 0.8937 - val_loss: 1.5273 - val_precision: 0.8718 - val_recall: 0.6538 - val_acc: 0.6789\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1797 - precision: 0.9796 - recall: 0.9600 - acc: 0.8937 - val_loss: 1.3921 - val_precision: 0.8222 - val_recall: 0.7115 - val_acc: 0.6881\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1858 - precision: 0.9797 - recall: 0.9667 - acc: 0.8949 - val_loss: 1.3562 - val_precision: 0.8182 - val_recall: 0.6923 - val_acc: 0.6789\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.2022 - precision: 0.9600 - recall: 0.9600 - acc: 0.8869 - val_loss: 1.3997 - val_precision: 0.8810 - val_recall: 0.7115 - val_acc: 0.7018\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1789 - precision: 0.9799 - recall: 0.9733 - acc: 0.9029 - val_loss: 1.4630 - val_precision: 0.8222 - val_recall: 0.7115 - val_acc: 0.7018\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1716 - precision: 1.0000 - recall: 0.9667 - acc: 0.9017 - val_loss: 1.5177 - val_precision: 0.8222 - val_recall: 0.7115 - val_acc: 0.6927\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1656 - precision: 0.9797 - recall: 0.9667 - acc: 0.9040 - val_loss: 1.6278 - val_precision: 0.8085 - val_recall: 0.7308 - val_acc: 0.6789\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1709 - precision: 0.9863 - recall: 0.9600 - acc: 0.8983 - val_loss: 1.7355 - val_precision: 0.8718 - val_recall: 0.6538 - val_acc: 0.6606\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1627 - precision: 0.9863 - recall: 0.9600 - acc: 0.9063 - val_loss: 1.5833 - val_precision: 0.8222 - val_recall: 0.7115 - val_acc: 0.6835\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1556 - precision: 0.9671 - recall: 0.9800 - acc: 0.8983 - val_loss: 1.6211 - val_precision: 0.8537 - val_recall: 0.6731 - val_acc: 0.6789\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1556 - precision: 0.9795 - recall: 0.9533 - acc: 0.9074 - val_loss: 1.6919 - val_precision: 0.8444 - val_recall: 0.7308 - val_acc: 0.6927\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1603 - precision: 0.9797 - recall: 0.9667 - acc: 0.9029 - val_loss: 1.7375 - val_precision: 0.8372 - val_recall: 0.6923 - val_acc: 0.6789\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1584 - precision: 0.9863 - recall: 0.9600 - acc: 0.8960 - val_loss: 1.7020 - val_precision: 0.8478 - val_recall: 0.7500 - val_acc: 0.6743\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1553 - precision: 0.9673 - recall: 0.9867 - acc: 0.9040 - val_loss: 1.8212 - val_precision: 0.8409 - val_recall: 0.7115 - val_acc: 0.6789\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1456 - precision: 0.9864 - recall: 0.9667 - acc: 0.9120 - val_loss: 1.8712 - val_precision: 0.8500 - val_recall: 0.6538 - val_acc: 0.6651\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1512 - precision: 0.9796 - recall: 0.9600 - acc: 0.8983 - val_loss: 1.9861 - val_precision: 0.8222 - val_recall: 0.7115 - val_acc: 0.6606\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1467 - precision: 0.9730 - recall: 0.9600 - acc: 0.9097 - val_loss: 1.9621 - val_precision: 0.8372 - val_recall: 0.6923 - val_acc: 0.6560\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1493 - precision: 0.9797 - recall: 0.9667 - acc: 0.9074 - val_loss: 2.0153 - val_precision: 0.8372 - val_recall: 0.6923 - val_acc: 0.6560\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1443 - precision: 0.9864 - recall: 0.9667 - acc: 0.9086 - val_loss: 2.0234 - val_precision: 0.8372 - val_recall: 0.6923 - val_acc: 0.6651\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1447 - precision: 0.9797 - recall: 0.9667 - acc: 0.9017 - val_loss: 2.0148 - val_precision: 0.8261 - val_recall: 0.7308 - val_acc: 0.6651\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1376 - precision: 0.9737 - recall: 0.9867 - acc: 0.9131 - val_loss: 2.1551 - val_precision: 0.8372 - val_recall: 0.6923 - val_acc: 0.6651\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1392 - precision: 0.9931 - recall: 0.9600 - acc: 0.9017 - val_loss: 2.1841 - val_precision: 0.8372 - val_recall: 0.6923 - val_acc: 0.6606\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1423 - precision: 0.9865 - recall: 0.9733 - acc: 0.9074 - val_loss: 2.0440 - val_precision: 0.8444 - val_recall: 0.7308 - val_acc: 0.6651\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1434 - precision: 0.9866 - recall: 0.9800 - acc: 0.9109 - val_loss: 1.8957 - val_precision: 0.8222 - val_recall: 0.7115 - val_acc: 0.6651\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1535 - precision: 0.9864 - recall: 0.9667 - acc: 0.9097 - val_loss: 1.8454 - val_precision: 0.8810 - val_recall: 0.7115 - val_acc: 0.7018\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1495 - precision: 0.9735 - recall: 0.9800 - acc: 0.9109 - val_loss: 2.0717 - val_precision: 0.8261 - val_recall: 0.7308 - val_acc: 0.6651\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1575 - precision: 0.9932 - recall: 0.9667 - acc: 0.9006 - val_loss: 1.9091 - val_precision: 0.8085 - val_recall: 0.7308 - val_acc: 0.6789\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1783 - precision: 0.9735 - recall: 0.9800 - acc: 0.8960 - val_loss: 1.7895 - val_precision: 0.8333 - val_recall: 0.6731 - val_acc: 0.6835\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1738 - precision: 0.9605 - recall: 0.9733 - acc: 0.8960 - val_loss: 2.3314 - val_precision: 0.8421 - val_recall: 0.6154 - val_acc: 0.6743\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1911 - precision: 0.9580 - recall: 0.9133 - acc: 0.8903 - val_loss: 1.8461 - val_precision: 0.8367 - val_recall: 0.7885 - val_acc: 0.6881\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.2073 - precision: 0.9388 - recall: 0.9200 - acc: 0.8857 - val_loss: 1.7825 - val_precision: 0.8750 - val_recall: 0.8077 - val_acc: 0.6789\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1946 - precision: 0.9474 - recall: 0.9600 - acc: 0.8857 - val_loss: 1.5862 - val_precision: 0.8542 - val_recall: 0.7885 - val_acc: 0.6972\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.2116 - precision: 0.9514 - recall: 0.9133 - acc: 0.8937 - val_loss: 1.5620 - val_precision: 0.8444 - val_recall: 0.7308 - val_acc: 0.6651\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.2057 - precision: 0.9790 - recall: 0.9333 - acc: 0.8994 - val_loss: 1.4270 - val_precision: 0.8077 - val_recall: 0.8077 - val_acc: 0.6789\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1840 - precision: 0.9662 - recall: 0.9533 - acc: 0.9097 - val_loss: 1.3961 - val_precision: 0.9286 - val_recall: 0.7500 - val_acc: 0.6835\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1573 - precision: 0.9732 - recall: 0.9667 - acc: 0.9051 - val_loss: 1.6495 - val_precision: 0.8810 - val_recall: 0.7115 - val_acc: 0.6881\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1532 - precision: 0.9930 - recall: 0.9467 - acc: 0.9154 - val_loss: 1.9553 - val_precision: 0.8431 - val_recall: 0.8269 - val_acc: 0.7064\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.2175 - precision: 0.9671 - recall: 0.9800 - acc: 0.9086 - val_loss: 1.4950 - val_precision: 0.9062 - val_recall: 0.5577 - val_acc: 0.6606\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.2068 - precision: 0.9603 - recall: 0.9667 - acc: 0.9040 - val_loss: 1.4497 - val_precision: 0.7586 - val_recall: 0.8462 - val_acc: 0.6789\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1560 - precision: 0.9930 - recall: 0.9467 - acc: 0.9029 - val_loss: 1.4594 - val_precision: 0.8667 - val_recall: 0.7500 - val_acc: 0.6743\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1435 - precision: 0.9932 - recall: 0.9667 - acc: 0.9086 - val_loss: 1.4978 - val_precision: 0.8511 - val_recall: 0.7692 - val_acc: 0.6835\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1365 - precision: 0.9733 - recall: 0.9733 - acc: 0.9143 - val_loss: 1.5407 - val_precision: 0.8696 - val_recall: 0.7692 - val_acc: 0.7018\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1382 - precision: 0.9866 - recall: 0.9800 - acc: 0.9051 - val_loss: 1.5457 - val_precision: 0.8571 - val_recall: 0.8077 - val_acc: 0.7110\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1370 - precision: 0.9800 - recall: 0.9800 - acc: 0.9166 - val_loss: 1.6395 - val_precision: 0.8333 - val_recall: 0.7692 - val_acc: 0.6927\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1500 - precision: 0.9737 - recall: 0.9867 - acc: 0.9097 - val_loss: 1.4984 - val_precision: 0.8367 - val_recall: 0.7885 - val_acc: 0.7156\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1508 - precision: 0.9733 - recall: 0.9733 - acc: 0.9086 - val_loss: 1.4110 - val_precision: 0.8400 - val_recall: 0.8077 - val_acc: 0.7202\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1383 - precision: 0.9799 - recall: 0.9733 - acc: 0.9074 - val_loss: 1.8074 - val_precision: 0.8478 - val_recall: 0.7500 - val_acc: 0.7018\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1393 - precision: 0.9867 - recall: 0.9867 - acc: 0.9074 - val_loss: 1.7153 - val_precision: 0.8367 - val_recall: 0.7885 - val_acc: 0.6927\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1393 - precision: 0.9799 - recall: 0.9733 - acc: 0.9029 - val_loss: 1.6223 - val_precision: 0.8696 - val_recall: 0.7692 - val_acc: 0.6972\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1304 - precision: 0.9932 - recall: 0.9733 - acc: 0.9154 - val_loss: 1.7688 - val_precision: 0.8571 - val_recall: 0.8077 - val_acc: 0.7018\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1332 - precision: 0.9737 - recall: 0.9867 - acc: 0.9086 - val_loss: 1.8766 - val_precision: 0.8511 - val_recall: 0.7692 - val_acc: 0.6835\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1327 - precision: 0.9735 - recall: 0.9800 - acc: 0.9074 - val_loss: 1.9520 - val_precision: 0.8667 - val_recall: 0.7500 - val_acc: 0.6835\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1275 - precision: 0.9932 - recall: 0.9733 - acc: 0.9189 - val_loss: 1.9767 - val_precision: 0.8696 - val_recall: 0.7692 - val_acc: 0.6881\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1288 - precision: 0.9800 - recall: 0.9800 - acc: 0.8983 - val_loss: 2.1134 - val_precision: 0.8333 - val_recall: 0.7692 - val_acc: 0.6789\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1280 - precision: 0.9799 - recall: 0.9733 - acc: 0.9120 - val_loss: 1.8438 - val_precision: 0.8696 - val_recall: 0.7692 - val_acc: 0.6927\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1315 - precision: 0.9932 - recall: 0.9667 - acc: 0.9109 - val_loss: 1.8416 - val_precision: 0.8478 - val_recall: 0.7500 - val_acc: 0.6927\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1311 - precision: 0.9803 - recall: 0.9933 - acc: 0.9017 - val_loss: 1.9839 - val_precision: 0.8696 - val_recall: 0.7692 - val_acc: 0.6927\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1304 - precision: 0.9799 - recall: 0.9733 - acc: 0.9051 - val_loss: 2.0174 - val_precision: 0.8667 - val_recall: 0.7500 - val_acc: 0.6789\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1258 - precision: 0.9931 - recall: 0.9600 - acc: 0.9086 - val_loss: 2.0439 - val_precision: 0.8696 - val_recall: 0.7692 - val_acc: 0.7018\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1249 - precision: 0.9735 - recall: 0.9800 - acc: 0.9234 - val_loss: 2.1272 - val_precision: 0.8864 - val_recall: 0.7500 - val_acc: 0.6743\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1321 - precision: 0.9800 - recall: 0.9800 - acc: 0.9029 - val_loss: 2.1286 - val_precision: 0.8837 - val_recall: 0.7308 - val_acc: 0.6743\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1224 - precision: 0.9932 - recall: 0.9733 - acc: 0.9223 - val_loss: 2.1194 - val_precision: 0.8667 - val_recall: 0.7500 - val_acc: 0.6972\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1268 - precision: 0.9735 - recall: 0.9800 - acc: 0.9006 - val_loss: 2.2108 - val_precision: 0.8723 - val_recall: 0.7885 - val_acc: 0.6927\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1255 - precision: 0.9801 - recall: 0.9867 - acc: 0.9223 - val_loss: 2.1918 - val_precision: 0.8696 - val_recall: 0.7692 - val_acc: 0.7018\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1300 - precision: 0.9799 - recall: 0.9733 - acc: 0.9051 - val_loss: 2.0464 - val_precision: 0.8889 - val_recall: 0.7692 - val_acc: 0.7018\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1222 - precision: 0.9932 - recall: 0.9733 - acc: 0.9131 - val_loss: 2.2161 - val_precision: 0.8511 - val_recall: 0.7692 - val_acc: 0.6972\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1304 - precision: 0.9733 - recall: 0.9733 - acc: 0.9097 - val_loss: 2.1712 - val_precision: 0.8200 - val_recall: 0.7885 - val_acc: 0.6972\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1303 - precision: 0.9671 - recall: 0.9800 - acc: 0.9051 - val_loss: 1.9682 - val_precision: 0.8864 - val_recall: 0.7500 - val_acc: 0.6835\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1253 - precision: 0.9801 - recall: 0.9867 - acc: 0.9154 - val_loss: 2.2020 - val_precision: 0.8511 - val_recall: 0.7692 - val_acc: 0.6972\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1277 - precision: 0.9675 - recall: 0.9933 - acc: 0.9109 - val_loss: 2.2351 - val_precision: 0.8810 - val_recall: 0.7115 - val_acc: 0.6881\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1323 - precision: 0.9664 - recall: 0.9600 - acc: 0.9029 - val_loss: 2.1182 - val_precision: 0.8810 - val_recall: 0.7115 - val_acc: 0.7018\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1268 - precision: 0.9800 - recall: 0.9800 - acc: 0.9120 - val_loss: 2.1430 - val_precision: 0.8511 - val_recall: 0.7692 - val_acc: 0.6789\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1233 - precision: 0.9675 - recall: 0.9933 - acc: 0.9029 - val_loss: 2.3659 - val_precision: 0.8696 - val_recall: 0.7692 - val_acc: 0.6881\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1221 - precision: 0.9737 - recall: 0.9867 - acc: 0.9246 - val_loss: 2.3236 - val_precision: 0.8667 - val_recall: 0.7500 - val_acc: 0.6927\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1253 - precision: 0.9799 - recall: 0.9733 - acc: 0.9143 - val_loss: 2.1644 - val_precision: 0.8837 - val_recall: 0.7308 - val_acc: 0.6927\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1231 - precision: 0.9799 - recall: 0.9733 - acc: 0.9074 - val_loss: 2.1801 - val_precision: 0.8511 - val_recall: 0.7692 - val_acc: 0.6789\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1208 - precision: 0.9800 - recall: 0.9800 - acc: 0.9109 - val_loss: 2.4405 - val_precision: 0.8333 - val_recall: 0.7692 - val_acc: 0.6789\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1203 - precision: 0.9804 - recall: 1.0000 - acc: 0.9177 - val_loss: 2.4133 - val_precision: 0.8723 - val_recall: 0.7885 - val_acc: 0.7018\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1198 - precision: 0.9865 - recall: 0.9733 - acc: 0.9223 - val_loss: 2.3692 - val_precision: 0.9000 - val_recall: 0.6923 - val_acc: 0.6789\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1281 - precision: 0.9799 - recall: 0.9733 - acc: 0.9040 - val_loss: 2.0825 - val_precision: 0.8571 - val_recall: 0.8077 - val_acc: 0.6972\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1271 - precision: 0.9675 - recall: 0.9933 - acc: 0.9109 - val_loss: 2.2135 - val_precision: 0.8542 - val_recall: 0.7885 - val_acc: 0.6881\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1289 - precision: 0.9865 - recall: 0.9733 - acc: 0.9029 - val_loss: 2.2069 - val_precision: 0.8444 - val_recall: 0.7308 - val_acc: 0.6651\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1237 - precision: 0.9800 - recall: 0.9800 - acc: 0.9109 - val_loss: 2.3600 - val_precision: 0.8478 - val_recall: 0.7500 - val_acc: 0.6927\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1616 - precision: 0.9801 - recall: 0.9867 - acc: 0.8994 - val_loss: 1.5817 - val_precision: 0.8605 - val_recall: 0.7115 - val_acc: 0.6789\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1939 - precision: 1.0000 - recall: 0.9667 - acc: 0.9029 - val_loss: 1.7654 - val_precision: 0.8478 - val_recall: 0.7500 - val_acc: 0.6835\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1508 - precision: 0.9739 - recall: 0.9933 - acc: 0.9051 - val_loss: 1.7127 - val_precision: 0.8511 - val_recall: 0.7692 - val_acc: 0.6972\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1446 - precision: 0.9737 - recall: 0.9867 - acc: 0.9154 - val_loss: 1.7320 - val_precision: 0.8333 - val_recall: 0.7692 - val_acc: 0.7064\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1367 - precision: 0.9671 - recall: 0.9800 - acc: 0.9131 - val_loss: 1.9386 - val_precision: 0.8696 - val_recall: 0.7692 - val_acc: 0.7064\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1390 - precision: 0.9733 - recall: 0.9733 - acc: 0.9017 - val_loss: 1.8503 - val_precision: 0.8864 - val_recall: 0.7500 - val_acc: 0.7064\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1293 - precision: 0.9735 - recall: 0.9800 - acc: 0.9086 - val_loss: 1.9743 - val_precision: 0.8864 - val_recall: 0.7500 - val_acc: 0.7018\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1222 - precision: 0.9865 - recall: 0.9733 - acc: 0.9120 - val_loss: 2.1153 - val_precision: 0.8837 - val_recall: 0.7308 - val_acc: 0.6881\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1212 - precision: 0.9735 - recall: 0.9800 - acc: 0.9166 - val_loss: 2.1536 - val_precision: 0.8511 - val_recall: 0.7692 - val_acc: 0.6927\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1246 - precision: 0.9866 - recall: 0.9800 - acc: 0.9200 - val_loss: 2.1890 - val_precision: 0.8511 - val_recall: 0.7692 - val_acc: 0.7018\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1217 - precision: 0.9932 - recall: 0.9667 - acc: 0.9109 - val_loss: 2.2026 - val_precision: 0.8667 - val_recall: 0.7500 - val_acc: 0.7018\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1201 - precision: 0.9671 - recall: 0.9800 - acc: 0.9131 - val_loss: 2.2784 - val_precision: 0.8696 - val_recall: 0.7692 - val_acc: 0.7064\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1183 - precision: 0.9867 - recall: 0.9867 - acc: 0.9120 - val_loss: 2.3749 - val_precision: 0.8696 - val_recall: 0.7692 - val_acc: 0.7018\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1183 - precision: 0.9865 - recall: 0.9733 - acc: 0.9143 - val_loss: 2.3951 - val_precision: 0.8889 - val_recall: 0.7692 - val_acc: 0.7064\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1176 - precision: 0.9803 - recall: 0.9933 - acc: 0.9086 - val_loss: 2.3229 - val_precision: 0.8889 - val_recall: 0.7692 - val_acc: 0.7064\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1198 - precision: 0.9675 - recall: 0.9933 - acc: 0.9154 - val_loss: 2.3347 - val_precision: 0.8696 - val_recall: 0.7692 - val_acc: 0.7018\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1170 - precision: 0.9675 - recall: 0.9933 - acc: 0.9143 - val_loss: 2.4335 - val_precision: 0.8636 - val_recall: 0.7308 - val_acc: 0.6927\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1183 - precision: 0.9865 - recall: 0.9733 - acc: 0.9166 - val_loss: 2.4621 - val_precision: 0.8696 - val_recall: 0.7692 - val_acc: 0.7064\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1171 - precision: 0.9803 - recall: 0.9933 - acc: 0.9246 - val_loss: 2.3070 - val_precision: 0.8333 - val_recall: 0.7692 - val_acc: 0.6927\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1203 - precision: 0.9803 - recall: 0.9933 - acc: 0.9200 - val_loss: 2.2981 - val_precision: 0.8667 - val_recall: 0.7500 - val_acc: 0.6927\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1186 - precision: 0.9739 - recall: 0.9933 - acc: 0.9154 - val_loss: 2.4223 - val_precision: 0.8864 - val_recall: 0.7500 - val_acc: 0.7064\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1290 - precision: 0.9864 - recall: 0.9667 - acc: 0.9166 - val_loss: 2.3179 - val_precision: 0.8889 - val_recall: 0.7692 - val_acc: 0.7110\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1218 - precision: 0.9801 - recall: 0.9867 - acc: 0.9177 - val_loss: 2.3964 - val_precision: 0.8511 - val_recall: 0.7692 - val_acc: 0.6972\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1231 - precision: 0.9737 - recall: 0.9867 - acc: 0.9189 - val_loss: 1.9474 - val_precision: 0.8696 - val_recall: 0.7692 - val_acc: 0.7018\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1277 - precision: 0.9735 - recall: 0.9800 - acc: 0.9120 - val_loss: 2.2429 - val_precision: 0.8511 - val_recall: 0.7692 - val_acc: 0.6881\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.3838 - precision: 0.8773 - recall: 0.9533 - acc: 0.8686 - val_loss: 1.8733 - val_precision: 0.8261 - val_recall: 0.7308 - val_acc: 0.6055\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.2785 - precision: 0.9701 - recall: 0.8667 - acc: 0.8686 - val_loss: 1.4627 - val_precision: 0.8462 - val_recall: 0.6346 - val_acc: 0.6881\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.2227 - precision: 0.9530 - recall: 0.9467 - acc: 0.8891 - val_loss: 1.6627 - val_precision: 0.8750 - val_recall: 0.6731 - val_acc: 0.6422\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1771 - precision: 0.9662 - recall: 0.9533 - acc: 0.9131 - val_loss: 1.9195 - val_precision: 0.9143 - val_recall: 0.6154 - val_acc: 0.6651\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1569 - precision: 0.9863 - recall: 0.9600 - acc: 0.9131 - val_loss: 1.9449 - val_precision: 0.8491 - val_recall: 0.8654 - val_acc: 0.7156\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1586 - precision: 0.9732 - recall: 0.9667 - acc: 0.9074 - val_loss: 1.5355 - val_precision: 0.8684 - val_recall: 0.6346 - val_acc: 0.6514\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1439 - precision: 0.9868 - recall: 0.9933 - acc: 0.9166 - val_loss: 1.6378 - val_precision: 0.8846 - val_recall: 0.8846 - val_acc: 0.7064\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1348 - precision: 0.9739 - recall: 0.9933 - acc: 0.9177 - val_loss: 1.9445 - val_precision: 0.8400 - val_recall: 0.8077 - val_acc: 0.6927\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1269 - precision: 0.9931 - recall: 0.9600 - acc: 0.9120 - val_loss: 2.1085 - val_precision: 0.8491 - val_recall: 0.8654 - val_acc: 0.6972\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1233 - precision: 0.9864 - recall: 0.9667 - acc: 0.9131 - val_loss: 2.0742 - val_precision: 0.8627 - val_recall: 0.8462 - val_acc: 0.6972\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1242 - precision: 0.9800 - recall: 0.9800 - acc: 0.9166 - val_loss: 2.2200 - val_precision: 0.8542 - val_recall: 0.7885 - val_acc: 0.6743\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1215 - precision: 0.9675 - recall: 0.9933 - acc: 0.9189 - val_loss: 2.2073 - val_precision: 0.8571 - val_recall: 0.8077 - val_acc: 0.6927\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1201 - precision: 0.9799 - recall: 0.9733 - acc: 0.9120 - val_loss: 2.3173 - val_precision: 0.8511 - val_recall: 0.7692 - val_acc: 0.6835\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1175 - precision: 0.9866 - recall: 0.9800 - acc: 0.9177 - val_loss: 2.3210 - val_precision: 0.8542 - val_recall: 0.7885 - val_acc: 0.6789\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1164 - precision: 1.0000 - recall: 0.9733 - acc: 0.9154 - val_loss: 2.4136 - val_precision: 0.8542 - val_recall: 0.7885 - val_acc: 0.6881\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1183 - precision: 0.9735 - recall: 0.9800 - acc: 0.9131 - val_loss: 2.5046 - val_precision: 0.8542 - val_recall: 0.7885 - val_acc: 0.6835\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1160 - precision: 0.9801 - recall: 0.9867 - acc: 0.9269 - val_loss: 2.4587 - val_precision: 0.8571 - val_recall: 0.8077 - val_acc: 0.6881\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1192 - precision: 0.9799 - recall: 0.9733 - acc: 0.9051 - val_loss: 2.5679 - val_precision: 0.8571 - val_recall: 0.8077 - val_acc: 0.6789\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1209 - precision: 0.9671 - recall: 0.9800 - acc: 0.9166 - val_loss: 2.0602 - val_precision: 0.8269 - val_recall: 0.8269 - val_acc: 0.6927\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1236 - precision: 0.9799 - recall: 0.9733 - acc: 0.9040 - val_loss: 2.2686 - val_precision: 0.8542 - val_recall: 0.7885 - val_acc: 0.6835\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1188 - precision: 0.9801 - recall: 0.9867 - acc: 0.9154 - val_loss: 2.5438 - val_precision: 0.8511 - val_recall: 0.7692 - val_acc: 0.6743\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1173 - precision: 0.9735 - recall: 0.9800 - acc: 0.9166 - val_loss: 2.6771 - val_precision: 0.8542 - val_recall: 0.7885 - val_acc: 0.6881\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1148 - precision: 0.9803 - recall: 0.9933 - acc: 0.9154 - val_loss: 2.6473 - val_precision: 0.8542 - val_recall: 0.7885 - val_acc: 0.6743\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1185 - precision: 0.9800 - recall: 0.9800 - acc: 0.9051 - val_loss: 2.5950 - val_precision: 0.8542 - val_recall: 0.7885 - val_acc: 0.6881\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1160 - precision: 0.9737 - recall: 0.9867 - acc: 0.9074 - val_loss: 2.6064 - val_precision: 0.8600 - val_recall: 0.8269 - val_acc: 0.6835\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1179 - precision: 0.9675 - recall: 0.9933 - acc: 0.9154 - val_loss: 2.6442 - val_precision: 0.8542 - val_recall: 0.7885 - val_acc: 0.6835\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1163 - precision: 0.9801 - recall: 0.9867 - acc: 0.9154 - val_loss: 2.7097 - val_precision: 0.8542 - val_recall: 0.7885 - val_acc: 0.6789\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1173 - precision: 0.9801 - recall: 0.9867 - acc: 0.9109 - val_loss: 2.5802 - val_precision: 0.8542 - val_recall: 0.7885 - val_acc: 0.6881\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1157 - precision: 0.9932 - recall: 0.9800 - acc: 0.9234 - val_loss: 2.6327 - val_precision: 0.8542 - val_recall: 0.7885 - val_acc: 0.6835\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1150 - precision: 1.0000 - recall: 0.9733 - acc: 0.9200 - val_loss: 2.7861 - val_precision: 0.8542 - val_recall: 0.7885 - val_acc: 0.6697\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1195 - precision: 0.9799 - recall: 0.9733 - acc: 0.9189 - val_loss: 2.8669 - val_precision: 0.8542 - val_recall: 0.7885 - val_acc: 0.6743\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1165 - precision: 0.9737 - recall: 0.9867 - acc: 0.9223 - val_loss: 2.8472 - val_precision: 0.8542 - val_recall: 0.7885 - val_acc: 0.6835\n",
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.1153 - precision: 0.9933 - recall: 0.9867 - acc: 0.9166 - val_loss: 2.5747 - val_precision: 0.8571 - val_recall: 0.8077 - val_acc: 0.6927\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=80)\n",
    "\n",
    "for mini_batch in range(200):\n",
    "        rnn_history = rnn_model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=1, batch_size=64, callbacks=[es])\n",
    "\n",
    "        precision = rnn_history.history['val_precision'][0]\n",
    "        recall = rnn_history.history['val_recall'][0]\n",
    "        f_score = (2.0 * precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'code': 'Personal advice', 'prob': 0.95512956},\n",
       " {'code': 'Negative Activating', 'prob': 0.043820683},\n",
       " {'code': 'Asking questions', 'prob': 0.000723947},\n",
       " {'code': 'Complementing, expressing appreciation', 'prob': 0.00031136753},\n",
       " {'code': 'Vocatives (addressing individual)', 'prob': 1.431781e-05}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_text(model, text):\n",
    "        predict = model.predict(pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=MAX_SEQUENCE_LENGTH)).ravel()\n",
    "        return sorted([{\"code\": factorized_codes_dict[idx], \"prob\": prob} for idx, prob in enumerate(predict)], reverse=True, key=lambda x: x['prob'])\n",
    "    \n",
    "predict_text(rnn_model, \"I think you should take it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.86      0.83        49\n",
      "           1       0.47      0.46      0.47        37\n",
      "           2       0.67      0.65      0.66        68\n",
      "           3       0.65      0.71      0.68        24\n",
      "           4       0.82      0.78      0.79        40\n",
      "\n",
      "    accuracy                           0.69       218\n",
      "   macro avg       0.68      0.69      0.69       218\n",
      "weighted avg       0.69      0.69      0.69       218\n",
      "\n",
      "Index(['Personal advice', 'Vocatives (addressing individual)',\n",
      "       'Complementing, expressing appreciation', 'Negative Activating',\n",
      "       'Asking questions'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_test = rnn_model.predict(x_val)\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(classification_report(y_test, y_val))\n",
    "print(factorized_codes_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model.save('./rnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"775pt\" viewBox=\"0.00 0.00 300.00 775.00\" width=\"300pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 771)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-771 296,-771 296,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 117562854424 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>117562854424</title>\n",
       "<polygon fill=\"none\" points=\"63.5,-657.5 63.5,-693.5 228.5,-693.5 228.5,-657.5 63.5,-657.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"146\" y=\"-671.8\">embedding_8: Embedding</text>\n",
       "</g>\n",
       "<!-- 117562855040 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>117562855040</title>\n",
       "<polygon fill=\"none\" points=\"93,-584.5 93,-620.5 199,-620.5 199,-584.5 93,-584.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"146\" y=\"-598.8\">lstm_15: LSTM</text>\n",
       "</g>\n",
       "<!-- 117562854424&#45;&gt;117562855040 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>117562854424-&gt;117562855040</title>\n",
       "<path d=\"M146,-657.4551C146,-649.3828 146,-639.6764 146,-630.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"149.5001,-630.5903 146,-620.5904 142.5001,-630.5904 149.5001,-630.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 117563323672 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>117563323672</title>\n",
       "<polygon fill=\"none\" points=\"93,-511.5 93,-547.5 199,-547.5 199,-511.5 93,-511.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"146\" y=\"-525.8\">lstm_16: LSTM</text>\n",
       "</g>\n",
       "<!-- 117562855040&#45;&gt;117563323672 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>117562855040-&gt;117563323672</title>\n",
       "<path d=\"M146,-584.4551C146,-576.3828 146,-566.6764 146,-557.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"149.5001,-557.5903 146,-547.5904 142.5001,-557.5904 149.5001,-557.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 117500721304 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>117500721304</title>\n",
       "<polygon fill=\"none\" points=\"0,-438.5 0,-474.5 292,-474.5 292,-438.5 0,-438.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"146\" y=\"-452.8\">attention_with_context_4: AttentionWithContext</text>\n",
       "</g>\n",
       "<!-- 117563323672&#45;&gt;117500721304 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>117563323672-&gt;117500721304</title>\n",
       "<path d=\"M146,-511.4551C146,-503.3828 146,-493.6764 146,-484.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"149.5001,-484.5903 146,-474.5904 142.5001,-484.5904 149.5001,-484.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 117500719568 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>117500719568</title>\n",
       "<polygon fill=\"none\" points=\"90,-365.5 90,-401.5 202,-401.5 202,-365.5 90,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"146\" y=\"-379.8\">dense_21: Dense</text>\n",
       "</g>\n",
       "<!-- 117500721304&#45;&gt;117500719568 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>117500721304-&gt;117500719568</title>\n",
       "<path d=\"M146,-438.4551C146,-430.3828 146,-420.6764 146,-411.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"149.5001,-411.5903 146,-401.5904 142.5001,-411.5904 149.5001,-411.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 117575624016 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>117575624016</title>\n",
       "<polygon fill=\"none\" points=\"78.5,-292.5 78.5,-328.5 213.5,-328.5 213.5,-292.5 78.5,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"146\" y=\"-306.8\">dropout_11: Dropout</text>\n",
       "</g>\n",
       "<!-- 117500719568&#45;&gt;117575624016 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>117500719568-&gt;117575624016</title>\n",
       "<path d=\"M146,-365.4551C146,-357.3828 146,-347.6764 146,-338.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"149.5001,-338.5903 146,-328.5904 142.5001,-338.5904 149.5001,-338.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 117575767712 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>117575767712</title>\n",
       "<polygon fill=\"none\" points=\"90,-219.5 90,-255.5 202,-255.5 202,-219.5 90,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"146\" y=\"-233.8\">dense_22: Dense</text>\n",
       "</g>\n",
       "<!-- 117575624016&#45;&gt;117575767712 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>117575624016-&gt;117575767712</title>\n",
       "<path d=\"M146,-292.4551C146,-284.3828 146,-274.6764 146,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"149.5001,-265.5903 146,-255.5904 142.5001,-265.5904 149.5001,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 117575625472 -->\n",
       "<g class=\"node\" id=\"node8\">\n",
       "<title>117575625472</title>\n",
       "<polygon fill=\"none\" points=\"78.5,-146.5 78.5,-182.5 213.5,-182.5 213.5,-146.5 78.5,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"146\" y=\"-160.8\">dropout_12: Dropout</text>\n",
       "</g>\n",
       "<!-- 117575767712&#45;&gt;117575625472 -->\n",
       "<g class=\"edge\" id=\"edge8\">\n",
       "<title>117575767712-&gt;117575625472</title>\n",
       "<path d=\"M146,-219.4551C146,-211.3828 146,-201.6764 146,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"149.5001,-192.5903 146,-182.5904 142.5001,-192.5904 149.5001,-192.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 117575862648 -->\n",
       "<g class=\"node\" id=\"node9\">\n",
       "<title>117575862648</title>\n",
       "<polygon fill=\"none\" points=\"90,-73.5 90,-109.5 202,-109.5 202,-73.5 90,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"146\" y=\"-87.8\">dense_23: Dense</text>\n",
       "</g>\n",
       "<!-- 117575625472&#45;&gt;117575862648 -->\n",
       "<g class=\"edge\" id=\"edge9\">\n",
       "<title>117575625472-&gt;117575862648</title>\n",
       "<path d=\"M146,-146.4551C146,-138.3828 146,-128.6764 146,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"149.5001,-119.5903 146,-109.5904 142.5001,-119.5904 149.5001,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 117576271464 -->\n",
       "<g class=\"node\" id=\"node10\">\n",
       "<title>117576271464</title>\n",
       "<polygon fill=\"none\" points=\"90,-.5 90,-36.5 202,-36.5 202,-.5 90,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"146\" y=\"-14.8\">dense_24: Dense</text>\n",
       "</g>\n",
       "<!-- 117575862648&#45;&gt;117576271464 -->\n",
       "<g class=\"edge\" id=\"edge10\">\n",
       "<title>117575862648-&gt;117576271464</title>\n",
       "<path d=\"M146,-73.4551C146,-65.3828 146,-55.6764 146,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"149.5001,-46.5903 146,-36.5904 142.5001,-46.5904 149.5001,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 117562854816 -->\n",
       "<g class=\"node\" id=\"node11\">\n",
       "<title>117562854816</title>\n",
       "<polygon fill=\"none\" points=\"95.5,-730.5 95.5,-766.5 196.5,-766.5 196.5,-730.5 95.5,-730.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"146\" y=\"-744.8\">117562854816</text>\n",
       "</g>\n",
       "<!-- 117562854816&#45;&gt;117562854424 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>117562854816-&gt;117562854424</title>\n",
       "<path d=\"M146,-730.4551C146,-722.3828 146,-712.6764 146,-703.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"149.5001,-703.5903 146,-693.5904 142.5001,-703.5904 149.5001,-703.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pydot as pyd\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import keras\n",
    "\n",
    "keras.utils.vis_utils.pydot = pyd\n",
    "\n",
    "SVG(model_to_dot(rnn_model).create(prog='dot', format='svg'))\n",
    "# rnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, Flatten, Dropout\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras.models import Model\n",
    "\n",
    "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index, trainable=False, extra_conv=True):\n",
    "    \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=trainable)\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    # Yoon Kim model (https://arxiv.org/abs/1408.5882)\n",
    "    convs = []\n",
    "    filter_sizes = [3,4,5]\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=128, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "        l_pool = MaxPooling1D(pool_size=3)(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = Concatenate(axis=1)(convs)\n",
    "\n",
    "    # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n",
    "    conv = Conv1D(filters=128, kernel_size=3, activation='relu')(embedded_sequences)\n",
    "    pool = MaxPooling1D(pool_size=3)(conv)\n",
    "\n",
    "    if extra_conv==True:\n",
    "        x = Dropout(0.5)(l_merge)  \n",
    "    else:\n",
    "        # Original Yoon Kim model\n",
    "        x = Dropout(0.5)(pool)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    #x = Dropout(0.5)(x)\n",
    "\n",
    "    preds = Dense(labels_index, activation='softmax')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/cl38824/anaconda2/envs/stan/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "factorized_codes = pd.factorize(df_cleaned[\"code\"])[0]\n",
    "factorized_codes_dict = pd.factorize(df_cleaned[\"code\"])[1]\n",
    "\n",
    "cnn_labels = to_categorical(np.asarray(factorized_codes))\n",
    "\n",
    "indices = np.arange(rnn_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "rnn_data = rnn_data[indices]\n",
    "cnn_labels = cnn_labels[indices]\n",
    "\n",
    "x_train = rnn_data[:-num_validation_samples]\n",
    "y_train = cnn_labels[:-num_validation_samples]\n",
    "x_val = rnn_data[-num_validation_samples:]\n",
    "y_val = cnn_labels[-num_validation_samples:]\n",
    "\n",
    "cnn_model = ConvNet(embedding_weights, MAX_SEQUENCE_LENGTH, len(word_index)+1, EMBEDDING_DIM, \n",
    "                len(list(df_cleaned[\"code\"].unique())), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=100, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 55)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_26 (Embedding)        (None, 55, 300)      640800      input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 53, 128)      115328      embedding_26[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 52, 128)      153728      embedding_26[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 51, 128)      192128      embedding_26[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 17, 128)      0           conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 17, 128)      0           conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 17, 128)      0           conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 51, 128)      0           max_pooling1d_36[0][0]           \n",
      "                                                                 max_pooling1d_37[0][0]           \n",
      "                                                                 max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 51, 128)      0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 6528)         0           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 128)          835712      flatten_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 5)            645         dense_19[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,938,341\n",
      "Trainable params: 1,297,541\n",
      "Non-trainable params: 640,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import print_summary, plot_model\n",
    "\n",
    "print_summary(cnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Personal advice', 'Vocatives (addressing individual)',\n",
       "       'Complementing, expressing appreciation', 'Negative Activating',\n",
       "       'Asking questions'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factorized_codes_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
