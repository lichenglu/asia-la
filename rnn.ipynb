{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Positive Activating</td>\n",
       "      <td>I found another interesting clue in your first...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Personal advice</td>\n",
       "      <td>It is good to see more details on this.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vocatives (addressing individual)</td>\n",
       "      <td>Hi Daniel,Thanks for your comments.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Positive Deactivating</td>\n",
       "      <td>This is cool!This confirms my personal impress...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Complementing, expressing appreciation</td>\n",
       "      <td>The final result is quite compelling and reall...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     code  \\\n",
       "0                     Positive Activating   \n",
       "1                         Personal advice   \n",
       "2       Vocatives (addressing individual)   \n",
       "3                   Positive Deactivating   \n",
       "4  Complementing, expressing appreciation   \n",
       "\n",
       "                                                text  \n",
       "0  I found another interesting clue in your first...  \n",
       "1            It is good to see more details on this.  \n",
       "2                Hi Daniel,Thanks for your comments.  \n",
       "3  This is cool!This confirms my personal impress...  \n",
       "4  The final result is quite compelling and reall...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "code\n",
       "Addresses or refers to the group using inclusive pronouns (addressing the whole group)     15\n",
       "Asking questions                                                                          180\n",
       "Complementing, expressing appreciation                                                    371\n",
       "Course reflection                                                                           2\n",
       "Expressing agreement                                                                       79\n",
       "Expressing disagreement                                                                    28\n",
       "Group cohesion                                                                              1\n",
       "Negative Activating                                                                       157\n",
       "Negative Deactivating                                                                      67\n",
       "Open communication                                                                          2\n",
       "Personal advice                                                                           202\n",
       "Phatics, salutations and greetings, Social sharing                                         66\n",
       "Positive Activating                                                                       220\n",
       "Positive Deactivating                                                                      30\n",
       "Quoting from others' messages & Referring explicitly to others' messages                   43\n",
       "Self-disclosure & expressing values                                                        57\n",
       "Unconventional emotion expression                                                          56\n",
       "Vocatives (addressing individual)                                                         183\n",
       "Name: text, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('code')['text'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "code\n",
       "Asking questions                          180\n",
       "Complementing, expressing appreciation    371\n",
       "Negative Activating                       157\n",
       "Personal advice                           202\n",
       "Vocatives (addressing individual)         183\n",
       "Name: text, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# over_50_cols = [\n",
    "#     'Asking questions', 'Complementing, expressing appreciation', 'Expressing agreement',\n",
    "#     'Negative Activating', 'Negative Deactivating', 'Personal advice',\n",
    "#     'Phatics, salutations and greetings, Social sharing', 'Positive Activating',\n",
    "#     'Self-disclosure & expressing values', 'Unconventional emotion expression', 'Vocatives (addressing individual)'\n",
    "# ]\n",
    "over_100_cols = [\n",
    "    'Asking questions', 'Complementing, expressing appreciation',\n",
    "    'Negative Activating', 'Personal advice',\n",
    "    'Vocatives (addressing individual)'\n",
    "]\n",
    "df_over_50 = df[df['code'].isin(over_100_cols)]\n",
    "df_over_50.groupby('code')['text'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.to_csv('./filtered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def standardize_text(df, text_field):\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\\S+\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\\S+\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]\", \" \")\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\", \"at\")\n",
    "    \n",
    "    df[text_field] = df[text_field].str.lower()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = standardize_text(df_over_50, 'text')\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>did you find any similarities about these two ...</td>\n",
       "      <td>[did, you, find, any, similarities, about, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>it would interesting to see how well this data...</td>\n",
       "      <td>[it, would, interesting, to, see, how, well, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>in which year does the population data start i...</td>\n",
       "      <td>[in, which, year, does, the, population, data,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>were they using a different data set?</td>\n",
       "      <td>[were, they, using, a, different, data, set]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>does this mean that there is no data for the g...</td>\n",
       "      <td>[does, this, mean, that, there, is, no, data, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>what is the large red circle in florida? why i...</td>\n",
       "      <td>[what, is, the, large, red, circle, in, florid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>does attending school directly make one literate?</td>\n",
       "      <td>[does, attending, school, directly, make, one,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>was uncertainty (prediction confidence interva...</td>\n",
       "      <td>[was, uncertainty, prediction, confidence, int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>have you thought of other data or known driver...</td>\n",
       "      <td>[have, you, thought, of, other, data, or, know...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>however, i don t understand where the focus on...</td>\n",
       "      <td>[however, i, don, t, understand, where, the, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>can an ecological situation impact to cancer d...</td>\n",
       "      <td>[can, an, ecological, situation, impact, to, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>maybe that is a lot to ask for this course, bu...</td>\n",
       "      <td>[maybe, that, is, a, lot, to, ask, for, this, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>was this the lite version of flourish, or was ...</td>\n",
       "      <td>[was, this, the, lite, version, of, flourish, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>is this still the case?</td>\n",
       "      <td>[is, this, still, the, case]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>did you try to arrange them in a different way?</td>\n",
       "      <td>[did, you, try, to, arrange, them, in, a, diff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>does that make it more difficult to get a view...</td>\n",
       "      <td>[does, that, make, it, more, difficult, to, ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>or it comes from your knowledge ?</td>\n",
       "      <td>[or, it, comes, from, your, knowledge]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>are these data perhaps influenced by a growing...</td>\n",
       "      <td>[are, these, data, perhaps, influenced, by, a,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>why did you highlight the specific nations tha...</td>\n",
       "      <td>[why, did, you, highlight, the, specific, nati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>wouldn t this then allow to judge a state s ab...</td>\n",
       "      <td>[wouldn, t, this, then, allow, to, judge, a, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>has its consumption been always the same?</td>\n",
       "      <td>[has, its, consumption, been, always, the, same]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>could you make a screen dump of the data tab i...</td>\n",
       "      <td>[could, you, make, a, screen, dump, of, the, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>why you are seeing these patterns?</td>\n",
       "      <td>[why, you, are, seeing, these, patterns]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>does this only include military personnel in u...</td>\n",
       "      <td>[does, this, only, include, military, personne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>this could be an interesting matter to debate ...</td>\n",
       "      <td>[this, could, be, an, interesting, matter, to,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>could i ask you what you mean by \"\"space out t...</td>\n",
       "      <td>[could, i, ask, you, what, you, mean, by, spac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>would that be too much?</td>\n",
       "      <td>[would, that, be, too, much]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>have you come up with some questions that you ...</td>\n",
       "      <td>[have, you, come, up, with, some, questions, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>which would work better to convey the erratic ...</td>\n",
       "      <td>[which, would, work, better, to, convey, the, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>did you actually follow this link?</td>\n",
       "      <td>[did, you, actually, follow, this, link]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1429</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>is that correct, or am i reading the chart wrong?</td>\n",
       "      <td>[is, that, correct, or, am, i, reading, the, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1447</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>can you tell me how you inserted those images ...</td>\n",
       "      <td>[can, you, tell, me, how, you, inserted, those...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>i m curious to know what if anything surprised...</td>\n",
       "      <td>[i, m, curious, to, know, what, if, anything, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1468</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>it could be linked to more awareness in the ge...</td>\n",
       "      <td>[it, could, be, linked, to, more, awareness, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1479</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>isn t the title of each sub chart enough?</td>\n",
       "      <td>[isn, t, the, title, of, each, sub, chart, eno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1509</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>if so, how do they differ from one another, or...</td>\n",
       "      <td>[if, so, how, do, they, differ, from, one, ano...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1517</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>did you find that any of your flourish charts ...</td>\n",
       "      <td>[did, you, find, that, any, of, your, flourish...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1523</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>did you try to make a map with your dataset?</td>\n",
       "      <td>[did, you, try, to, make, a, map, with, your, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1556</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>are co2 emissions lower in the countries where...</td>\n",
       "      <td>[are, co2, emissions, lower, in, the, countrie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1559</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>why is 30 longer than 1200?</td>\n",
       "      <td>[why, is, 30, longer, than, 1200]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>will those areas be able to support the aging ...</td>\n",
       "      <td>[will, those, areas, be, able, to, support, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1569</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>what were your impressions of the data set and...</td>\n",
       "      <td>[what, were, your, impressions, of, the, data,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1584</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>what did you want to show us?</td>\n",
       "      <td>[what, did, you, want, to, show, us]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1589</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>maybe even put that into context of the gdp (h...</td>\n",
       "      <td>[maybe, even, put, that, into, context, of, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>if b corp does have too much of an american co...</td>\n",
       "      <td>[if, b, corp, does, have, too, much, of, an, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1606</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>how would you represent it on graphic itself, ...</td>\n",
       "      <td>[how, would, you, represent, it, on, graphic, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1621</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>are people from the southern countries more ha...</td>\n",
       "      <td>[are, people, from, the, southern, countries, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1638</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>a question i have is there a significance to t...</td>\n",
       "      <td>[a, question, i, have, is, there, a, significa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1651</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>may i share the link?</td>\n",
       "      <td>[may, i, share, the, link]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1652</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>from the data set, is there any sense of regio...</td>\n",
       "      <td>[from, the, data, set, is, there, any, sense, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1658</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>do they represent number of seats in parliamen...</td>\n",
       "      <td>[do, they, represent, number, of, seats, in, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1666</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>for example, is china using more on recreation...</td>\n",
       "      <td>[for, example, is, china, using, more, on, rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1670</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>did you look at the link and scroll down and s...</td>\n",
       "      <td>[did, you, look, at, the, link, and, scroll, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1691</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>where does this data come from?</td>\n",
       "      <td>[where, does, this, data, come, from]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1692</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>are these numbers affecting the current long w...</td>\n",
       "      <td>[are, these, numbers, affecting, the, current,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1724</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>might there be a better solution than to use a...</td>\n",
       "      <td>[might, there, be, a, better, solution, than, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1729</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>i have a question  were you using the free ver...</td>\n",
       "      <td>[i, have, a, question, were, you, using, the, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1730</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>what is the map actually showing? why not use ...</td>\n",
       "      <td>[what, is, the, map, actually, showing, why, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1734</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>in google trends, were is the cheesecake?</td>\n",
       "      <td>[in, google, trends, were, is, the, cheesecake]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1735</th>\n",
       "      <td>Asking questions</td>\n",
       "      <td>why has the life expectancy until 2009 of viet...</td>\n",
       "      <td>[why, has, the, life, expectancy, until, 2009,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  code                                               text  \\\n",
       "6     Asking questions  did you find any similarities about these two ...   \n",
       "15    Asking questions  it would interesting to see how well this data...   \n",
       "16    Asking questions  in which year does the population data start i...   \n",
       "33    Asking questions              were they using a different data set?   \n",
       "37    Asking questions  does this mean that there is no data for the g...   \n",
       "41    Asking questions  what is the large red circle in florida? why i...   \n",
       "42    Asking questions  does attending school directly make one literate?   \n",
       "48    Asking questions  was uncertainty (prediction confidence interva...   \n",
       "54    Asking questions  have you thought of other data or known driver...   \n",
       "82    Asking questions  however, i don t understand where the focus on...   \n",
       "97    Asking questions  can an ecological situation impact to cancer d...   \n",
       "100   Asking questions  maybe that is a lot to ask for this course, bu...   \n",
       "111   Asking questions  was this the lite version of flourish, or was ...   \n",
       "114   Asking questions                            is this still the case?   \n",
       "116   Asking questions    did you try to arrange them in a different way?   \n",
       "117   Asking questions  does that make it more difficult to get a view...   \n",
       "118   Asking questions                  or it comes from your knowledge ?   \n",
       "131   Asking questions  are these data perhaps influenced by a growing...   \n",
       "135   Asking questions  why did you highlight the specific nations tha...   \n",
       "146   Asking questions  wouldn t this then allow to judge a state s ab...   \n",
       "153   Asking questions          has its consumption been always the same?   \n",
       "161   Asking questions  could you make a screen dump of the data tab i...   \n",
       "167   Asking questions                 why you are seeing these patterns?   \n",
       "173   Asking questions  does this only include military personnel in u...   \n",
       "195   Asking questions  this could be an interesting matter to debate ...   \n",
       "210   Asking questions  could i ask you what you mean by \"\"space out t...   \n",
       "212   Asking questions                            would that be too much?   \n",
       "220   Asking questions  have you come up with some questions that you ...   \n",
       "226   Asking questions  which would work better to convey the erratic ...   \n",
       "233   Asking questions                 did you actually follow this link?   \n",
       "...                ...                                                ...   \n",
       "1429  Asking questions  is that correct, or am i reading the chart wrong?   \n",
       "1447  Asking questions  can you tell me how you inserted those images ...   \n",
       "1454  Asking questions  i m curious to know what if anything surprised...   \n",
       "1468  Asking questions  it could be linked to more awareness in the ge...   \n",
       "1479  Asking questions          isn t the title of each sub chart enough?   \n",
       "1509  Asking questions  if so, how do they differ from one another, or...   \n",
       "1517  Asking questions  did you find that any of your flourish charts ...   \n",
       "1523  Asking questions       did you try to make a map with your dataset?   \n",
       "1556  Asking questions  are co2 emissions lower in the countries where...   \n",
       "1559  Asking questions                        why is 30 longer than 1200?   \n",
       "1564  Asking questions  will those areas be able to support the aging ...   \n",
       "1569  Asking questions  what were your impressions of the data set and...   \n",
       "1584  Asking questions                      what did you want to show us?   \n",
       "1589  Asking questions  maybe even put that into context of the gdp (h...   \n",
       "1598  Asking questions  if b corp does have too much of an american co...   \n",
       "1606  Asking questions  how would you represent it on graphic itself, ...   \n",
       "1621  Asking questions  are people from the southern countries more ha...   \n",
       "1638  Asking questions  a question i have is there a significance to t...   \n",
       "1651  Asking questions                              may i share the link?   \n",
       "1652  Asking questions  from the data set, is there any sense of regio...   \n",
       "1658  Asking questions  do they represent number of seats in parliamen...   \n",
       "1666  Asking questions  for example, is china using more on recreation...   \n",
       "1670  Asking questions  did you look at the link and scroll down and s...   \n",
       "1691  Asking questions                    where does this data come from?   \n",
       "1692  Asking questions  are these numbers affecting the current long w...   \n",
       "1724  Asking questions  might there be a better solution than to use a...   \n",
       "1729  Asking questions  i have a question  were you using the free ver...   \n",
       "1730  Asking questions  what is the map actually showing? why not use ...   \n",
       "1734  Asking questions          in google trends, were is the cheesecake?   \n",
       "1735  Asking questions  why has the life expectancy until 2009 of viet...   \n",
       "\n",
       "                                                 tokens  \n",
       "6     [did, you, find, any, similarities, about, the...  \n",
       "15    [it, would, interesting, to, see, how, well, t...  \n",
       "16    [in, which, year, does, the, population, data,...  \n",
       "33         [were, they, using, a, different, data, set]  \n",
       "37    [does, this, mean, that, there, is, no, data, ...  \n",
       "41    [what, is, the, large, red, circle, in, florid...  \n",
       "42    [does, attending, school, directly, make, one,...  \n",
       "48    [was, uncertainty, prediction, confidence, int...  \n",
       "54    [have, you, thought, of, other, data, or, know...  \n",
       "82    [however, i, don, t, understand, where, the, f...  \n",
       "97    [can, an, ecological, situation, impact, to, c...  \n",
       "100   [maybe, that, is, a, lot, to, ask, for, this, ...  \n",
       "111   [was, this, the, lite, version, of, flourish, ...  \n",
       "114                        [is, this, still, the, case]  \n",
       "116   [did, you, try, to, arrange, them, in, a, diff...  \n",
       "117   [does, that, make, it, more, difficult, to, ge...  \n",
       "118              [or, it, comes, from, your, knowledge]  \n",
       "131   [are, these, data, perhaps, influenced, by, a,...  \n",
       "135   [why, did, you, highlight, the, specific, nati...  \n",
       "146   [wouldn, t, this, then, allow, to, judge, a, s...  \n",
       "153    [has, its, consumption, been, always, the, same]  \n",
       "161   [could, you, make, a, screen, dump, of, the, d...  \n",
       "167            [why, you, are, seeing, these, patterns]  \n",
       "173   [does, this, only, include, military, personne...  \n",
       "195   [this, could, be, an, interesting, matter, to,...  \n",
       "210   [could, i, ask, you, what, you, mean, by, spac...  \n",
       "212                        [would, that, be, too, much]  \n",
       "220   [have, you, come, up, with, some, questions, t...  \n",
       "226   [which, would, work, better, to, convey, the, ...  \n",
       "233            [did, you, actually, follow, this, link]  \n",
       "...                                                 ...  \n",
       "1429  [is, that, correct, or, am, i, reading, the, c...  \n",
       "1447  [can, you, tell, me, how, you, inserted, those...  \n",
       "1454  [i, m, curious, to, know, what, if, anything, ...  \n",
       "1468  [it, could, be, linked, to, more, awareness, i...  \n",
       "1479  [isn, t, the, title, of, each, sub, chart, eno...  \n",
       "1509  [if, so, how, do, they, differ, from, one, ano...  \n",
       "1517  [did, you, find, that, any, of, your, flourish...  \n",
       "1523  [did, you, try, to, make, a, map, with, your, ...  \n",
       "1556  [are, co2, emissions, lower, in, the, countrie...  \n",
       "1559                  [why, is, 30, longer, than, 1200]  \n",
       "1564  [will, those, areas, be, able, to, support, th...  \n",
       "1569  [what, were, your, impressions, of, the, data,...  \n",
       "1584               [what, did, you, want, to, show, us]  \n",
       "1589  [maybe, even, put, that, into, context, of, th...  \n",
       "1598  [if, b, corp, does, have, too, much, of, an, a...  \n",
       "1606  [how, would, you, represent, it, on, graphic, ...  \n",
       "1621  [are, people, from, the, southern, countries, ...  \n",
       "1638  [a, question, i, have, is, there, a, significa...  \n",
       "1651                         [may, i, share, the, link]  \n",
       "1652  [from, the, data, set, is, there, any, sense, ...  \n",
       "1658  [do, they, represent, number, of, seats, in, p...  \n",
       "1666  [for, example, is, china, using, more, on, rec...  \n",
       "1670  [did, you, look, at, the, link, and, scroll, d...  \n",
       "1691              [where, does, this, data, come, from]  \n",
       "1692  [are, these, numbers, affecting, the, current,...  \n",
       "1724  [might, there, be, a, better, solution, than, ...  \n",
       "1729  [i, have, a, question, were, you, using, the, ...  \n",
       "1730  [what, is, the, map, actually, showing, why, n...  \n",
       "1734    [in, google, trends, were, is, the, cheesecake]  \n",
       "1735  [why, has, the, life, expectancy, until, 2009,...  \n",
       "\n",
       "[180 rows x 3 columns]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned[df_cleaned['code'] == 'Asking questions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cl38824/anaconda2/envs/stan/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "df_cleaned[\"tokens\"] = df_cleaned[\"text\"].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13386 words total, with a vocabulary size of 2135\n",
      "Max sentence length is 55\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "all_words = [word for tokens in df_cleaned[\"tokens\"] for word in tokens]\n",
    "sentence_lengths = [len(tokens) for tokens in df_cleaned[\"tokens\"]]\n",
    "VOCAB = sorted(list(set(all_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_words), len(VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "word2vec_path = \"GoogleNews-vectors-negative300.bin.gz\"\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
    "    if len(tokens_list) < 1:\n",
    "        return np.zeros(k)\n",
    "    if generate_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged\n",
    "\n",
    "def get_word2vec_embeddings(vectors, clean_questions, generate_missing=False):\n",
    "    embeddings = clean_questions['tokens'].apply(lambda x: get_average_word2vec(x, vectors, \n",
    "                                                                                generate_missing=generate_missing))\n",
    "    return list(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = get_word2vec_embeddings(word2vec, df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras as K\n",
    "from keras.layers import *\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "    \n",
    "\n",
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "    Note: The layer has been tested with Keras 2.0.6\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2135 unique tokens.\n",
      "(2136, 300)\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "MAX_SEQUENCE_LENGTH = max(sentence_lengths)\n",
    "VOCAB_SIZE = len(VOCAB)\n",
    "\n",
    "VALIDATION_SPLIT=.2\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(df_cleaned[\"text\"].tolist())\n",
    "sequences = tokenizer.texts_to_sequences(df_cleaned[\"text\"].tolist())\n",
    "\n",
    "rnn_data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "factorized_codes = pd.factorize(df_cleaned[\"code\"])[0]\n",
    "factorized_codes_dict = pd.factorize(df_cleaned[\"code\"])[1]\n",
    "\n",
    "labels = factorized_codes\n",
    "\n",
    "indices = np.arange(rnn_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "rnn_data = rnn_data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * rnn_data.shape[0])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "embedding_weights = np.zeros((len(word_index)+1, EMBEDDING_DIM))\n",
    "for word, index in word_index.items():\n",
    "    embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "print(embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def rnn_text(embeddings, max_sequence_length, num_words, embedding_dim, labels_index): \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_words,\n",
    "                        embedding_dim,\n",
    "                        weights=[embeddings],\n",
    "                        input_length=max_sequence_length,\n",
    "                        mask_zero=True,\n",
    "                        trainable=False))\n",
    "    # model.add(Bidirectional(LSTM(100)))\n",
    "    model.add(LSTM(128, return_sequences=True))\n",
    "    model.add(LSTM(64, return_sequences=True))\n",
    "    model.add(AttentionWithContext())\n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(32, activation=\"relu\"))\n",
    "    model.add(Dense(labels_index, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = rnn_data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = rnn_data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "\n",
    "rnn_model = rnn_text(\n",
    "    embedding_weights, \n",
    "    MAX_SEQUENCE_LENGTH, \n",
    "    len(word_index)+1, \n",
    "    EMBEDDING_DIM, \n",
    "    len(list(df_cleaned[\"code\"].unique()))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 875 samples, validate on 218 samples\n",
      "Epoch 1/100\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1126 - acc: 0.9166 - val_loss: 3.0942 - val_acc: 0.6651\n",
      "Epoch 2/100\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1123 - acc: 0.9143 - val_loss: 3.0737 - val_acc: 0.6697\n",
      "Epoch 3/100\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1141 - acc: 0.9097 - val_loss: 3.0120 - val_acc: 0.6835\n",
      "Epoch 4/100\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1124 - acc: 0.9120 - val_loss: 2.9247 - val_acc: 0.6743\n",
      "Epoch 5/100\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1121 - acc: 0.9200 - val_loss: 2.9188 - val_acc: 0.6789\n",
      "Epoch 6/100\n",
      "875/875 [==============================] - 2s 2ms/step - loss: 0.1122 - acc: 0.9223 - val_loss: 2.9265 - val_acc: 0.6743\n",
      "Epoch 7/100\n",
      "875/875 [==============================] - 2s 2ms/step - loss: 0.1123 - acc: 0.9189 - val_loss: 2.9398 - val_acc: 0.6743\n",
      "Epoch 8/100\n",
      "875/875 [==============================] - 2s 2ms/step - loss: 0.1120 - acc: 0.9211 - val_loss: 2.9718 - val_acc: 0.6697\n",
      "Epoch 9/100\n",
      "875/875 [==============================] - 2s 2ms/step - loss: 0.1126 - acc: 0.9166 - val_loss: 2.9419 - val_acc: 0.6881\n",
      "Epoch 10/100\n",
      "875/875 [==============================] - 2s 2ms/step - loss: 0.1131 - acc: 0.9074 - val_loss: 2.9372 - val_acc: 0.6835\n",
      "Epoch 11/100\n",
      "875/875 [==============================] - 2s 2ms/step - loss: 0.1131 - acc: 0.9063 - val_loss: 2.9266 - val_acc: 0.6972\n",
      "Epoch 12/100\n",
      "875/875 [==============================] - 2s 2ms/step - loss: 0.1130 - acc: 0.9074 - val_loss: 2.9327 - val_acc: 0.6835\n",
      "Epoch 13/100\n",
      "875/875 [==============================] - 2s 2ms/step - loss: 0.1123 - acc: 0.9166 - val_loss: 2.9400 - val_acc: 0.6789\n",
      "Epoch 14/100\n",
      "875/875 [==============================] - 2s 2ms/step - loss: 0.1135 - acc: 0.9177 - val_loss: 2.9837 - val_acc: 0.6743\n",
      "Epoch 15/100\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1124 - acc: 0.9269 - val_loss: 2.9904 - val_acc: 0.6789\n",
      "Epoch 16/100\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1125 - acc: 0.9200 - val_loss: 2.9867 - val_acc: 0.6743\n",
      "Epoch 17/100\n",
      "875/875 [==============================] - 2s 2ms/step - loss: 0.1127 - acc: 0.9189 - val_loss: 3.0006 - val_acc: 0.6835\n",
      "Epoch 18/100\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1118 - acc: 0.9177 - val_loss: 3.0349 - val_acc: 0.6743\n",
      "Epoch 19/100\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1116 - acc: 0.9200 - val_loss: 3.0485 - val_acc: 0.6789\n",
      "Epoch 20/100\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1124 - acc: 0.9211 - val_loss: 3.0439 - val_acc: 0.6835\n",
      "Epoch 21/100\n",
      "875/875 [==============================] - 2s 2ms/step - loss: 0.1117 - acc: 0.9189 - val_loss: 3.0326 - val_acc: 0.6743\n",
      "Epoch 22/100\n",
      "875/875 [==============================] - 2s 2ms/step - loss: 0.1136 - acc: 0.9166 - val_loss: 3.0427 - val_acc: 0.6835\n",
      "Epoch 23/100\n",
      "875/875 [==============================] - 2s 2ms/step - loss: 0.1122 - acc: 0.9303 - val_loss: 3.0661 - val_acc: 0.6835\n",
      "Epoch 24/100\n",
      "875/875 [==============================] - 2s 2ms/step - loss: 0.1125 - acc: 0.9189 - val_loss: 3.0777 - val_acc: 0.6835\n",
      "Epoch 25/100\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1128 - acc: 0.9120 - val_loss: 3.0939 - val_acc: 0.6881\n",
      "Epoch 26/100\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1133 - acc: 0.9086 - val_loss: 3.0957 - val_acc: 0.6835\n",
      "Epoch 27/100\n",
      "875/875 [==============================] - 2s 2ms/step - loss: 0.1122 - acc: 0.9120 - val_loss: 3.0995 - val_acc: 0.6881\n",
      "Epoch 28/100\n",
      "875/875 [==============================] - 2s 2ms/step - loss: 0.1116 - acc: 0.9269 - val_loss: 3.1191 - val_acc: 0.6835\n",
      "Epoch 29/100\n",
      "875/875 [==============================] - 2s 2ms/step - loss: 0.1130 - acc: 0.9109 - val_loss: 3.1114 - val_acc: 0.6743\n",
      "Epoch 30/100\n",
      "875/875 [==============================] - 2s 3ms/step - loss: 0.1135 - acc: 0.9074 - val_loss: 3.1162 - val_acc: 0.6789\n",
      "Epoch 31/100\n",
      "875/875 [==============================] - 2s 2ms/step - loss: 0.1125 - acc: 0.9154 - val_loss: 3.0327 - val_acc: 0.6743\n",
      "Epoch 32/100\n",
      "875/875 [==============================] - 2s 2ms/step - loss: 0.1121 - acc: 0.9246 - val_loss: 3.0258 - val_acc: 0.6651\n",
      "Epoch 33/100\n",
      "875/875 [==============================] - 2s 2ms/step - loss: 0.1124 - acc: 0.9223 - val_loss: 3.0093 - val_acc: 0.6697\n",
      "Epoch 34/100\n",
      "875/875 [==============================] - 2s 2ms/step - loss: 0.1124 - acc: 0.9154 - val_loss: 3.0234 - val_acc: 0.6697\n",
      "Epoch 35/100\n",
      "875/875 [==============================] - 2s 2ms/step - loss: 0.1119 - acc: 0.9200 - val_loss: 3.0568 - val_acc: 0.6743\n",
      "Epoch 00035: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=30)\n",
    "rnn_fit = rnn_model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=100, batch_size=64, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'code': 'Complementing, expressing appreciation', 'prob': 0.9998149},\n",
       " {'code': 'Negative Activating', 'prob': 0.00015111157},\n",
       " {'code': 'Personal advice', 'prob': 3.071309e-05},\n",
       " {'code': 'Vocatives (addressing individual)', 'prob': 3.2058363e-06},\n",
       " {'code': 'Asking questions', 'prob': 2.707606e-08}]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_text(model, text):\n",
    "        predict = model.predict(pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=MAX_SEQUENCE_LENGTH)).ravel()\n",
    "        return sorted([{\"code\": factorized_codes_dict[idx], \"prob\": prob} for idx, prob in enumerate(predict)], reverse=True, key=lambda x: x['prob'])\n",
    "    \n",
    "predict_text(rnn_model, \"I don't like this feeling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model.save('./rnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218/218 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.0567792052522713, 0.674311926058673]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pydot as pyd\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import keras\n",
    "\n",
    "keras.utils.vis_utils.pydot = pyd\n",
    "\n",
    "SVG(model_to_dot(rnn_model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, Flatten, Dropout\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras.models import Model\n",
    "\n",
    "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index, trainable=False, extra_conv=True):\n",
    "    \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=trainable)\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    # Yoon Kim model (https://arxiv.org/abs/1408.5882)\n",
    "    convs = []\n",
    "    filter_sizes = [3,4,5]\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=128, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "        l_pool = MaxPooling1D(pool_size=3)(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = Concatenate(axis=1)(convs)\n",
    "\n",
    "    # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n",
    "    conv = Conv1D(filters=128, kernel_size=3, activation='relu')(embedded_sequences)\n",
    "    pool = MaxPooling1D(pool_size=3)(conv)\n",
    "\n",
    "    if extra_conv==True:\n",
    "        x = Dropout(0.5)(l_merge)  \n",
    "    else:\n",
    "        # Original Yoon Kim model\n",
    "        x = Dropout(0.5)(pool)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    #x = Dropout(0.5)(x)\n",
    "\n",
    "    preds = Dense(labels_index, activation='softmax')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/cl38824/anaconda2/envs/stan/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "factorized_codes = pd.factorize(df_cleaned[\"code\"])[0]\n",
    "factorized_codes_dict = pd.factorize(df_cleaned[\"code\"])[1]\n",
    "\n",
    "cnn_labels = to_categorical(np.asarray(factorized_codes))\n",
    "\n",
    "indices = np.arange(rnn_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "rnn_data = rnn_data[indices]\n",
    "cnn_labels = cnn_labels[indices]\n",
    "\n",
    "x_train = rnn_data[:-num_validation_samples]\n",
    "y_train = cnn_labels[:-num_validation_samples]\n",
    "x_val = rnn_data[-num_validation_samples:]\n",
    "y_val = cnn_labels[-num_validation_samples:]\n",
    "\n",
    "cnn_model = ConvNet(embedding_weights, MAX_SEQUENCE_LENGTH, len(word_index)+1, EMBEDDING_DIM, \n",
    "                len(list(df_cleaned[\"code\"].unique())), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=100, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 55)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_26 (Embedding)        (None, 55, 300)      640800      input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 53, 128)      115328      embedding_26[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 52, 128)      153728      embedding_26[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 51, 128)      192128      embedding_26[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 17, 128)      0           conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 17, 128)      0           conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 17, 128)      0           conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 51, 128)      0           max_pooling1d_36[0][0]           \n",
      "                                                                 max_pooling1d_37[0][0]           \n",
      "                                                                 max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 51, 128)      0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 6528)         0           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 128)          835712      flatten_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 5)            645         dense_19[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,938,341\n",
      "Trainable params: 1,297,541\n",
      "Non-trainable params: 640,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import print_summary, plot_model\n",
    "\n",
    "print_summary(cnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1093, 5)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[24, 4, 21, 93]]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences([\"thank you very much!\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
